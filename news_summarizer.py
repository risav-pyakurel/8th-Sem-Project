# -*- coding: utf-8 -*-
"""news-summarizer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vGyprzge_JVZ40v5pul9FfBl1WhiBXIK
"""

# Install necessary libraries
!pip install pandas nltk scikit-learn transformers datasets rouge_score

# Importing necessary libraries
import numpy as np
import pandas as pd
import nltk
import re
import torch
import datasets
from rouge import Rouge
import matplotlib.pyplot as plt
from datasets import DatasetDict
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    DataCollatorForSeq2Seq,
)
from huggingface_hub import notebook_login
from sklearn.model_selection import train_test_split

# Download necessary NLTK resources
nltk.download('punkt')  # Download 'punkt' tokenizer
nltk.download('wordnet')  # Required for lemmatization
nltk.download('punkt_tab')

# Mount Google Drive to access the files
from google.colab import drive
drive.mount('/content/drive')

# Setting device (GPU if available)
device = "cuda" if torch.cuda.is_available() else "cpu"
print(device)

# Load the dataset from Google Drive paths
df = pd.read_csv("/content/drive/MyDrive/news_summary.csv", encoding='iso-8859-1')
df_raw = pd.read_csv("/content/drive/MyDrive/news_summary_more.csv", encoding='iso-8859-1')

# Check dataset structure
print(df.head())

# Clean the dataset
def data_cleaning(column, column_name=""):
    data = []
    lemmatizer = WordNetLemmatizer()
    for row in column:
        row = re.sub("(\\t)", ' ', str(row)).lower()  # remove escape characters
        row = re.sub("(\\r)", ' ', str(row)).lower()
        row = re.sub("(\\n)", ' ', str(row)).lower()
        row = re.sub("(__+)", ' ', str(row)).lower()  # remove _ if it occurs consecutively
        row = re.sub("(--+)", ' ', str(row)).lower()  # remove - if it occurs consecutively
        row = re.sub("(~~+)", ' ', str(row)).lower()  # remove ~ if it occurs consecutively
        row = re.sub("(\+\++)", ' ', str(row)).lower()  # remove + if it occurs consecutively
        row = re.sub("(\.\.+)", ' ', str(row)).lower()  # remove . if it occurs consecutively
        row = re.sub(r"[<>()|&©ø\[\]\'\;~*]", ' ', str(row)).lower()  # remove special characters
        row = re.sub("(mailto:)", ' ', str(row)).lower()  # remove mailto:
        row = re.sub(r"(\\x9\d)", ' ', str(row)).lower()  # remove \x9* in text
        row = re.sub("([iI][nN][cC]\d+)", 'INC_NUM', str(row)).lower()  # replace INC nums to INC_NUM
        row = re.sub("([cC][mM]\d+)|([cC][hH][gG]\d+)", 'CM_NUM', str(row)).lower()  # replace CM# and CHG# to CM_NUM
        row = re.sub("(\s+)", ' ', str(row)).lower()  # remove multiple spaces
        tokens = word_tokenize(row)
        lemmatized_row = ' '.join([lemmatizer.lemmatize(token) for token in tokens])
        data.append(lemmatized_row)

    return pd.DataFrame({column_name: data})

# Apply cleaning
brief_cleaning1 = data_cleaning(df['text'], column_name='text')
brief_cleaning2 = data_cleaning(df['ctext'], column_name='summary')  # Corrected to use 'ctext' as summary

# Prepare the cleaned dataset
cleaned_complete_dataset = pd.DataFrame()
cleaned_complete_dataset['text'] = brief_cleaning1['text']
cleaned_complete_dataset['summary'] = brief_cleaning2['summary']  # This will now have 'ctext' as summary

# Filter out long texts or summaries
max_text_len = 2500
max_summary_len = 70
short_text = []
short_summary = []

for i in range(len(cleaned_complete_dataset)):
    if len(cleaned_complete_dataset['summary'][i].split()) <= max_summary_len and len(cleaned_complete_dataset['text'][i].split()) <= max_text_len:
        short_text.append(cleaned_complete_dataset['text'][i])
        short_summary.append(cleaned_complete_dataset['summary'][i])

cleaned_complete_dataset = pd.DataFrame({'text': short_text, 'summary': short_summary})

# Train-test split
train_df, test_df = train_test_split(cleaned_complete_dataset, test_size=0.1, shuffle=True)

# Convert to HuggingFace dataset
train_df = datasets.Dataset.from_pandas(train_df)
test_df = datasets.Dataset.from_pandas(test_df)

# Importing the necessary library
from huggingface_hub import login

# Login with your Hugging Face token
token = ""
login(token)

# Updated Model setup with authentication for access
model_name = 't5-small'  # Changed model name to a commonly available model
tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=token)

# Model setup
T5model = AutoModelForSeq2SeqLM.from_pretrained(model_name, use_auth_token=token).to(device)

# Training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="t5-news",  # Where to save the model and training outputs
    evaluation_strategy="epoch",  # Evaluation at the end of each epoch
    learning_rate=2e-5,  # Learning rate for the optimizer
    per_device_train_batch_size=4,  # Batch size for training
    per_device_eval_batch_size=4,  # Batch size for evaluation
    weight_decay=0.01,  # Weight decay for regularization
    save_total_limit=2,  # Limit the number of saved checkpoints
    num_train_epochs=4,  # Number of epochs to train the model
    predict_with_generate=True,  # Use generate for predictions
    fp16=True,  # Enable mixed precision training
    report_to="none"  # Disable reporting to external platforms
)

# Step after Chunk 7: Define the `prepare_dataset` function for tokenization
def prepare_dataset(data):
    # Tokenize inputs (text)
    inputs = data["text"]
    model_inputs = tokenizer(inputs, max_length=512, truncation=True)

    # Tokenize labels (summary)
    labels = tokenizer(text_target=data["summary"], max_length=128, truncation=True)

    # Add tokenized labels as 'labels' key in the model_inputs dictionary
    model_inputs["labels"] = labels["input_ids"]

    return model_inputs

# Step after Chunk 7: Apply the tokenization process using the `prepare_dataset` function
tokenized_data = train_val_test_dataset.map(prepare_dataset, batched=True)

# Check the columns of the dataset
print(train_val_test_dataset.column_names)

# Tokenize the dataset using the correct column names
def prepare_dataset(examples):
    # Tokenize the input and output texts
    model_inputs = tokenizer(examples['text'], max_length=512, truncation=True)
    labels = tokenizer(examples['summary'], max_length=150, truncation=True)
    # Replace padding token ID with -100 to ignore the padding tokens in the loss calculation
    labels["input_ids"] = [[-100 if token == tokenizer.pad_token_id else token for token in example] for example in labels["input_ids"]]
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Apply the tokenization process using the `prepare_dataset` function
tokenized_data = train_val_test_dataset.map(prepare_dataset, batched=True)

# Define data collator for padding dynamically during training
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_name)

# Define a function to compute ROUGE metrics
from rouge import Rouge

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    # Decode predictions and labels
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Compute ROUGE scores
    rouge = Rouge()
    result = rouge.get_scores(decoded_preds, decoded_labels, avg=True, ignore_empty=True)
    return result

# Load the pre-trained model from Hugging Face
from transformers import AutoModelForSeq2SeqLM

T5model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)

# Define training arguments
from transformers import Seq2SeqTrainingArguments

training_args = Seq2SeqTrainingArguments(
    output_dir="t5-news",  # Output directory for model checkpoints
    evaluation_strategy="epoch",  # Evaluate once every epoch
    learning_rate=2e-5,  # Learning rate
    per_device_train_batch_size=4,  # Training batch size per device
    per_device_eval_batch_size=4,  # Evaluation batch size per device
    weight_decay=0.01,  # Weight decay for regularization
    save_total_limit=2,  # Limit number of saved checkpoints
    num_train_epochs=4,  # Number of training epochs
    predict_with_generate=True,  # Enable generation during evaluation
    fp16=True,  # Use mixed precision (float16)
    report_to="none"  # Disable reporting to external platforms
)

from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    model=T5model,
    args=training_args,
    train_dataset=tokenized_data["train"],
    eval_dataset=tokenized_data["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

trainer.train()

# Save the trained model and tokenizer
T5model.save_pretrained("t5-news-final")
tokenizer.save_pretrained("t5-news-final")

# Use the model for inference (summarization)
from transformers import pipeline

pipe = pipeline("summarization", model="t5-news-final", tokenizer=tokenizer, device=device)

# Example of summarization
temp = pipe(["China have won the first gold medal of Paris 2024 Olympics. Chinese shooters Sheng Lihao and Huang Yuting defeated South Korea's Keum Ji-hyeon and Park Ha-jun 16-12 to bag gold medal in 10m Air Rifle mixed team event."])

print(temp[0]['summary_text'])

!pip install evaluate

import evaluate
import numpy as np

# Initialize the ROUGE metric
rouge = evaluate.load("rouge")

# Decode predictions and references
predicted_summaries = tokenizer.batch_decode(test_predictions.predictions, skip_special_tokens=True)

# Filter out the padding tokens (-100) from the labels
labels = np.where(test_predictions.label_ids != -100, test_predictions.label_ids, tokenizer.pad_token_id)
reference_summaries = tokenizer.batch_decode(labels, skip_special_tokens=True)

# Compute ROUGE scores
rouge_result = rouge.compute(predictions=predicted_summaries, references=reference_summaries)

# Print the ROUGE scores
print("ROUGE scores on the test dataset:")
for key, value in rouge_result.items():
    print(f"{key}: {value}")

